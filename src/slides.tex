\begin{frame}
    \frametitle{Feed Forward Neural Network}
\includegraphics[width=0.7\textwidth]{simple_feedforward_network}
\end{frame}


\begin{frame}
\frametitle{State of hidden neurons}
\[
  \eta_{i} =
  \Phi \left(
    \sum_{j=1}^{n}
    w_{i,j}\,x_j + \text{bias}_{i,1}
  \right),
\]
\end{frame}

\begin{frame}
\frametitle{State of output neurons}
\[
  \gamma_{i} =
  \Phi\left(
    \sum_{j=1}^{h}
    \delta_{i,j}\,\eta_i + \text{bias}_{i,2}
  \right),
\]
\end{frame}

\begin{frame}
    \frametitle{Input-hidden weights}
  \[
    W_{h \times {n+1}} =
    \left[
      \begin{array}{ccc|c}
        w_{1, 1} & \cdots & w_{1, n} & w_{1, n+1} \\
        \vdots   & \ddots & \vdots   & \vdots     \\
        w_{h, 1} & \cdots & w_{h, n} & w_{h, n+1}
      \end{array}
    \right],
  \]
\end{frame}

\begin{frame}
    \frametitle{Hidden-output weights}
  \[
    \Delta_{m \times h+1} =
    \left[
      \begin{array}{ccc|c}
        \delta_{1,1} & \cdots & \delta_{1,h} & \delta_{1,h+1}\\
        \vdots & \ddots &  \vdots & \vdots\\
        \delta_{m,1} & \cdots & \delta_{m,h} &\delta_{m,h+1}
      \end{array}
    \right],
  \]
\end{frame}

\begin{frame}
\frametitle{Arquitecture of the solution}
\includegraphics[width=0.7\textwidth]{arq}
\end{frame}


\begin{frame}
      \frametitle{CUBLAS}
      \lstinputlisting{src/cublas.cu}
\end{frame}

\begin{frame}
      \frametitle{CUDA kernels}
      \lstinputlisting{src/kernels.cu}
\end{frame}

\begin{frame}
      \frametitle{Takeaways}
\begin{itemize}
\item Activation function time is negligible (~2\%).
\item It is not worth the effort to integrate 4 operations into one kernel.
\item Only fine tuning needed: keep occupancy high.
\item Separation into simple components makes testing easier.
\item Hard limit met easily by the GPU, not by the CPU.
\end{itemize}
\end{frame}



